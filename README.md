# ğŸ¯ Proyecto: AutomatizaciÃ³n basada en visiÃ³n + entrada de texto/voz

## ğŸ§  Objetivo

Desarrollar un sistema que, dada una orden (escrita o hablada), sea capaz de:

1. **Capturar la pantalla**
2. **Analizarla visualmente** (GPT-4 Vision, manual o API)
3. **Determinar quÃ© hacer** (clic, escribir, etc.)
4. **Ejecutar esa acciÃ³n en pantalla**

Inicialmente trabajamos por **mÃ³dulos separados** que luego uniremos de forma **manual** y finalmente **automÃ¡tica**.

---

## ğŸ“¦ Estructura del proyecto

```bash
project-root/
â”‚
â”œâ”€â”€ input/                   # Entradas manuales: texto o voz
â”‚   â”œâ”€â”€ text_orders/         # Archivos de texto con Ã³rdenes
â”‚   â””â”€â”€ voice_clips/         # Audios a transcribir (WAV, MP3)
â”‚
â”œâ”€â”€ parsed_steps/           # JSONs con pasos desglosados desde input
â”‚
â”œâ”€â”€ screenshots/            # Capturas de pantalla tomadas manual o automÃ¡ticamente
â”‚
â”œâ”€â”€ vision_outputs/         # Output de GPT Vision: coordenadas o instrucciones por imagen
â”‚
â”œâ”€â”€ executions/             # Capturas despuÃ©s de ejecutar acciones
â”‚
â”œâ”€â”€ scripts/                # Todos los mÃ³dulos funcionales
â”‚   â”œâ”€â”€ text_to_steps.py
â”‚   â”œâ”€â”€ voice_to_text.py
â”‚   â”œâ”€â”€ screenshot.py
â”‚   â”œâ”€â”€ vision_prompt.py
â”‚   â”œâ”€â”€ execute_actions.py
â”‚   â””â”€â”€ orchestrate.py
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
## âš™ï¸ MÃ³dulos funcionales

1. Entrada
text_to_steps.py
Toma una orden en texto y genera una lista de pasos en JSON.

voice_to_text.py
Transcribe una orden de voz usando Whisper o similar y guarda .txt.

2. Captura de pantalla
screenshot.py
Captura la pantalla o permite al usuario seleccionar un Ã¡rea. Guarda .png.

3. VisiÃ³n artificial (manual con GPT-4V)
vision_prompt.py
Proporciona una plantilla de prompt para subir manualmente a ChatGPT.
El usuario pega luego el JSON de respuesta.

4. EjecuciÃ³n automÃ¡tica
execute_actions.py
Recibe coordenadas desde un JSON y simula clics o escritura con pyautogui.

5. Orquestador (semiautomÃ¡tico / automÃ¡tico)
orchestrate.py
Conecta todas las partes en secuencia. Al principio manual, luego automÃ¡tico.

## ğŸ” Flujo de trabajo modular (manual)

Escribir o grabar una orden â†’ input/

Convertir a pasos con text_to_steps.py â†’ parsed_steps/

Capturar pantalla con screenshot.py â†’ screenshots/

Subir imagen + pasos manualmente a GPT-4V â†’ guardar .json en vision_outputs/

Ejecutar acciones con execute_actions.py â†’ logs en executions/

## ğŸ”§ Requisitos

Python 3.9+

Paquetes (instalar con pip install -r requirements.txt)

pyautogui

openai

speechrecognition

pydub

whisper (si se usa entrada por voz)

pillow

## ğŸ§ª Modo debug

Cada mÃ³dulo puede ejecutarse de forma independiente con la bandera --debug para validar entrada/salida.

bash
Copiar
Editar
python scripts/text_to_steps.py --input input/text_orders/example.txt --debug

## ğŸš€ Objetivo de integraciÃ³n (semana 2)

La idea es que todo el sistema funcione con un Ãºnico comando:

bash
Copiar
Editar
python scripts/orchestrate.py --input "Abre Google y busca clima en Madrid"
Que internamente haga: entrada â†’ pasos â†’ screenshot â†’ GPT â†’ coordenadas â†’ ejecuciÃ³n.

## ğŸ“„ Licencia
MIT â€“ Uso libre para fines de desarrollo, estudio y mejora de automatizaciÃ³n con visiÃ³n + LLM.

## ğŸ‘¥ Equipo
Proyecto desarrollado por 3 integrantes en 2 semanas. Cada mÃ³dulo fue testeado individualmente y luego integrado paso a paso.

