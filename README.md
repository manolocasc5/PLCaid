# PLCaid: Agente de AutomatizaciÃ³n y GeneraciÃ³n de CÃ³digo SCL

## ğŸ’¡ DescripciÃ³n del Proyecto

**PLCaid** es un prototipo de agente de inteligencia artificial diseÃ±ado para automatizar la configuraciÃ³n de entornos de PLC y generar cÃ³digo de control en lenguaje SCL (Structured Control Language) basado en instrucciones dadas en lenguaje natural.

El objetivo es simplificar flujos de trabajo complejos de automatizaciÃ³n industrial, como la configuraciÃ³n de TIA Portal, utilizando visiÃ³n por computadora, bases de datos vectoriales y modelos de lenguaje grande.

## âš™ï¸ Arquitectura de la ImplementaciÃ³n Actual

El proyecto opera bajo un flujo de trabajo **secuencial** que combina dos estrategias de localizaciÃ³n de UI:

1.  **Entrada del Usuario:** El usuario introduce una orden por texto o voz a travÃ©s de la interfaz web de Streamlit.
2.  **Fase de NavegaciÃ³n Flexible (`steps.json`):** El sistema traduce descripciones de texto (`"Abrir la vista del proyecto"`) a vectores (embeddings) y los busca en **Qdrant** para encontrar la imagen de UI mÃ¡s relevante.
3.  **GeneraciÃ³n de CÃ³digo:** La orden del usuario se envÃ­a a la API de **OpenAI (GPT-4)**, que genera el cÃ³digo SCL optimizado para PLC/HMI.
4.  **Fase de EjecuciÃ³n de CÃ³digo y FinalizaciÃ³n (`steps2.json`):** El sistema utiliza rutas de imagen fijas (`../capture/i8.png`) y **PyAutoGUI** para pegar el cÃ³digo SCL en la interfaz y completar los pasos finales (compilar, guardar).

---

## ğŸ“¦ Estructura del proyecto

```bash
project-root/
â”‚
â”œâ”€â”€ input/                   # Entradas manuales: texto o voz
â”‚   â”œâ”€â”€ text_orders/         # Archivos de texto con Ã³rdenes
â”‚   â””â”€â”€ voice_clips/         # Audios a transcribir (WAV, MP3)
â”‚
â”œâ”€â”€ parsed_steps/           # JSONs con pasos desglosados desde input
â”‚
â”œâ”€â”€ screenshots/            # Capturas de pantalla tomadas manual o automÃ¡ticamente
â”‚
â”œâ”€â”€ vision_outputs/         # Output de GPT Vision: coordenadas o instrucciones por imagen
â”‚
â”œâ”€â”€ executions/             # Capturas despuÃ©s de ejecutar acciones
â”‚
â”œâ”€â”€ scripts/                # Todos los mÃ³dulos funcionales
â”‚   â”œâ”€â”€ text_to_steps.py
â”‚   â”œâ”€â”€ voice_to_text.py
â”‚   â”œâ”€â”€ screenshot.py
â”‚   â”œâ”€â”€ vision_prompt.py
â”‚   â”œâ”€â”€ execute_actions.py
â”‚   â””â”€â”€ orchestrate.py
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸš€ VisiÃ³n y Futuras Implementaciones

Para transformar PLCaid en un agente autÃ³nomo y robusto, la hoja de ruta incluye la adopciÃ³n de arquitecturas de agentes avanzadas y la migraciÃ³n de tareas a modelos locales.

### 1. Arquitectura Multiagente (LangGraph)

* **ImplementaciÃ³n:** Utilizar un *framework* como **LangGraph** para orquestar agentes especializados.
* **Valor AÃ±adido:** El sistema pasarÃ¡ de un guion lineal a un flujo dinÃ¡mico, permitiendo el **manejo inteligente de errores** y la **recuperaciÃ³n autÃ³noma** si un elemento de UI cambia o no aparece.

### 2. MigraciÃ³n a LLMs Multimodales y Locales

* **LLMs Multimodales:** Integrar modelos avanzados (GPT-4o / Gemini 1.5 Pro) para un **razonamiento visual** mÃ¡s profundo, permitiendo al agente comprender el contexto de la UI en su totalidad (no solo buscando un elemento especÃ­fico).
* **Uso Local:** Evaluar el uso de modelos pequeÃ±os y eficientes (e.g., LLaMA 3, Mistral) en local para reducir la latencia, los costos y la dependencia de APIs externas.

### 3. IntegraciÃ³n de OCR y RAG

* **Herramientas de OCR (con OpenCV):** Utilizar **OpenCV** y motores de OCR (como Tesseract) para mejorar la **fiabilidad en la lectura de texto pequeÃ±o** en la UI, como etiquetas y mensajes de error, complementando al LLM.
* **RAG (GeneraciÃ³n Aumentada con RecuperaciÃ³n):** Extender el uso de **Qdrant** para indexar manuales y documentaciÃ³n de PLC. Esto permitirÃ¡ que el agente genere cÃ³digo SCL con informaciÃ³n actualizada, superando las limitaciones de la fecha de corte de entrenamiento de los LLMs.

---

## âš™ï¸ InstalaciÃ³n y ConfiguraciÃ³n

1.  **Clonar el repositorio:**
    ```bash
    git clone [URL_DEL_REPOSITORIO]
    cd PLCaid
    ```

2.  **Crear y activar el entorno virtual:**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # En Linux/macOS
    .\.venv\Scripts\activate  # En Windows
    ```

3.  **Instalar dependencias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configurar variables de entorno:**
    Crea un archivo llamado **`.env`** en la raÃ­z del proyecto y aÃ±ade tus claves de API y configuraciones de Qdrant:
    ```env
    OPENAI_API_KEY="sk-..."
    GEMINI_API_KEY="AIza..."
    QDRANT_URL="[Tu URL de Qdrant o 'localhost']"
    QDRANT_API_KEY="[Tu clave de Qdrant si usas la nube]"
    COLLECTION_NAME="plc_ui_vectors"
    ```

5.  **Indexar las imÃ¡genes:**
    Ejecuta el script de indexaciÃ³n para llenar la base de datos de Qdrant con los embeddings de las capturas de UI:
    ```bash
    python image_indexer.py
    ```

---

## ğŸš€ EjecuciÃ³n del Proyecto

Para iniciar la interfaz de usuario:

```bash
streamlit run interface.py
```

Una vez iniciada la interfaz, puedes escribir o grabar una orden en lenguaje natural y seleccionar el monitor donde se encuentra el entorno de PLC activo.

## ğŸ“„ Licencia
MIT â€“ Uso libre para fines de desarrollo, estudio y mejora de automatizaciÃ³n con visiÃ³n + LLM.

## ğŸ‘¥ Equipo
Proyecto desarrollado por 3 integrantes en 2 semanas. Cada mÃ³dulo fue testeado individualmente y luego integrado paso a paso.

